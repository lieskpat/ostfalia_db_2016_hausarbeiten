# 1. Einleitung
Seit es Computer gibt, gibt es auch die Problematik der Datenhaltung. Diese reicht von der einfachen Ablage der Daten in Dateien in einem entsprechenden Dateisystem bis zu hochkomplexen Datenstrukturen in unterschiedlichen Systemen, um die relevanten Daten hochverfügbar vorhalten zu können. Dabei waren und sind Datenbanken von herausragender Bedeutung. Eine wichtige Rolle spielten bisher die relationalen Datenbanken, welche zum heutigen Zeitpunkt sehr ausgereift sind und streng nach dem so genannten ACID-Prinzip arbeiten. Bei (Meier 2013) heißt es dazu:
> „Die vier Begriffe Atomarität (Atomicity), Konsistenz (Consistency), Isolation (Isolation) und Dauerhaftigkeit (Durability) beschreiben das so genannte ACID-Prinzip einer Transaktion. Dieses ist für Datenbanksysteme grundlegend und garantiert jedem Anwender, konsistente Datenbankzustände in ebensolche überführen zu können.“ (Meier 2013, S. 99)

Zum besseren Verständnis ist die weitergehende Erläuterung der Begriffe interessant. Zur Atomarität wird dargelegt, dass eine Transaktion entweder ganz oder gar nicht ausgeführt wird. Bezüglich der Widerspruchsfreiheit der Daten wird betont, dass bei Transaktionsende alle Konsistenzbedingungen erfüllt sein müssen. Die Isolation solle vor Seiteneffekten schützen, also eine Transaktion wie eine Einzelbenutzeraktion erscheinen lassen. Und die Dauerhaftigkeit setzt eine uneingeschränkte Rekonstruierbarkeit voraus. (vgl. Meier 2013, S. 98–99)
Mit dem Schwerpunkt auf der strikten Gewährleistung der Konsistenz gibt es aber immer häufiger Probleme. Meier und Kaufmann konstatieren dazu in ihrem Buch „ SQL- & NoSQL-Datenbanken“:
> „Es gibt allerdings [...] Anwendungsszenarien, in denen die Transaktions- und Konsistenzorientierung SQL-basierter Datenbanken im Weg ist, beispielsweise bei der Anforderung an hochperformante Verarbeitung umfangreicher Datenmengen.“ (Meier und Kaufmann 2016, S. 187)

Dies zielt vor allem auf die rasante Entwicklung des Internets und der damit verbundenen Entstehung neuer und Verlagerung bestehender Anwendungen in selbiges ab, in denen teilweise enorme Datenmengen unter Nutzung Verteilter Systeme bewältigt werden müssen, wie sie beispielsweise beim Cloud-Computing Verwendung finden.
Dabei ist zu unterscheiden, ob es sich um ein System handelt, auf welches nur lesend zugegriffen wird oder ob auch ein schreibender Zugriff ermöglicht werden soll. Für ersteres, zum Beispiel eine in sich geschlossene Enzyklopädie, die zentral gepflegt wird und deren Inhalt letztlich nur zum Konsum angeboten wird, gibt es einfache Mechanismen. Hier werden zur hohen Verfügbarkeit in einem verteilten System von mehreren Servern einfach einheitlich Repliken der jeweils aktuellen Version angeboten, welche bei Bedarf auf einen neuen Stand gebracht werden. Da Änderungen nur an einem zentralen System vorgenommen werden, kann hierbei natürlich die Konsistenz des Datenbestandes jederzeit gewährleistet werden.
Anders sieht es im zweiten Fall aus. Wenn verschiedene Nutzer einen schreibenden Zugriff auf ein und denselben Datenbestand vornehmen wollen, kann es unweigerlich zu Konflikten kommen. Das aus dieser Konstellation Probleme erwachsen, ist nicht weiter verwunderlich. Ein allgemein anerkannter Ansatz zur Betrachtung dieser Probleme ist das so genannte CAP-Theorem, welches im folgenden Abschnitt erläutert wird. Später wird dann noch ein Blick auf Versuche zur Lösung der Problematik geworfen.
